var documenterSearchIndex = {"docs":
[{"location":"layers/bayesian/#Bayesian-Layers","page":"Bayesian","title":"Bayesian Layers","text":"","category":"section"},{"location":"layers/bayesian/","page":"Bayesian","title":"Bayesian","text":"Pages=[\"bayesian.md\"]","category":"page"},{"location":"layers/bayesian/","page":"Bayesian","title":"Bayesian","text":"We follow the paper \"Weight Uncertainty in Neural Networks\" to implement Bayesian Layers. The paper proposes replacing point estimates of weights in the weight matric with distributions. So instead of optimizing for weights directly, we optimize the parameters of the distributions from which we sample weights. We sample a different set of weights at every forward pass. The predictive uncertainty is given by approximating the integral over the weight distribution using Monte Carlo sampling. ","category":"page"},{"location":"layers/bayesian/","page":"Bayesian","title":"Bayesian","text":"The first component of creating Bayesian layers is trainable distributions, we use DistributionsAD to backprop through distributions using Zygote, the flux autodiff framework.  A trainable distribution should be a subtype of the type AbstractTrainableDist","category":"page"},{"location":"layers/bayesian/","page":"Bayesian","title":"Bayesian","text":"DeepUncertainty.initializers","category":"page"},{"location":"layers/mclayers/#MC-Layers","page":"MC Dropout","title":"MC Layers","text":"","category":"section"},{"location":"layers/mclayers/","page":"MC Dropout","title":"MC Dropout","text":"Pages=[\"mclayers.md\"]","category":"page"},{"location":"layers/mclayers/","page":"MC Dropout","title":"MC Dropout","text":"The goal of MC Layers is to facilitate converting any layer defined in Flux into its Bayesian counterpart. MC Dropout is a principled tecnique to estimate predictive uncertainty in neural networs. The paper Dropout as a Bayesian Approximation proves that a neural network with dropout applied before every layer is equivalent to a Deep Gaussain Process. We can apply dropout at test time to simulate functions draws in a Gaussian Process for approximate Variational Inference. ","category":"page"},{"location":"layers/mclayers/","page":"MC Dropout","title":"MC Dropout","text":"In simple terms, a network with dropout applied before every layer has a different weight configuration in every forward pass (Note that we also apply dropout at test time, usually dropout is turned off during test time to ensemble the various weight samples implicitly). Each weight configuration can be considered an independent sample from the underlying weight distribution. The mean and variance of these sample predictions are the mean and variance of the predictive distribution. Higher the variance, more the uncertianty. ","category":"page"},{"location":"layers/mclayers/","page":"MC Dropout","title":"MC Dropout","text":"DeepUncertainty.MCLayer","category":"page"},{"location":"layers/mclayers/#DeepUncertainty.MCLayer","page":"MC Dropout","title":"DeepUncertainty.MCLayer","text":"MCLayer(layer, dropout)\n\nA generic Monte Carlo dropout layer. Takes in any \"traditional\" flux  layer and a function that implements dropout. Performs the usual layer  forward pass and then passes the acitvations through the given dropout function.  Let x be the input array, the forward pass then becomes – dropout(layer(x))\n\nFields\n\nlayer: Traditional Flux layer \ndropout: A function that implements dropout \n\n\n\n\n\n","category":"type"},{"location":"layers/mclayers/","page":"MC Dropout","title":"MC Dropout","text":"We also implement MC versions of Conv and Dense layers as a drop-in replacement. ","category":"page"},{"location":"layers/mclayers/","page":"MC Dropout","title":"MC Dropout","text":"DeepUncertainty.MCDense\nDeepUncertainty.MCConv ","category":"page"},{"location":"layers/mclayers/#DeepUncertainty.MCDense","page":"MC Dropout","title":"DeepUncertainty.MCDense","text":"MCDense(in::Int, out::Int, dropout_rate, σ=identity; bias=true, init=glorot_uniform)\nMCDense(layer, dropout_rate)\n\nCreates a traditional dense layer with MC dropout functionality.  MC Dropout simply means that dropout is activated in both train and test times  Reference - Dropout as a bayesian approximation - https://arxiv.org/abs/1506.02142 \n\nThe traditional dense layer is a field in the struct MCDense, so all the  arguments required for the dense layer can be provided, or the layer can  be provided too. The forward pass is the affine transformation of the dense layer followed by dropout applied on the resulting activations. \n\ny = dropout(σ.(W * x .+ bias), dropout_rate)\n\nFields\n\nlayer: A traditional dense layer \ndropout: A function that implements dropout  \n\nArguments\n\nin::Integer: Input dimension of features \nout::Integer: Output dimension of features \ndropout_rate::AbstractFloat: Dropout rate \nσ::F=identity: Activation function, defaults to identity\ninit=glorot_normal: Initialization function, defaults to glorot_normal \n\n\n\n\n\n","category":"function"},{"location":"layers/mclayers/#DeepUncertainty.MCConv","page":"MC Dropout","title":"DeepUncertainty.MCConv","text":"MCConv(filter, in => out, σ = identity;\n        stride = 1, pad = 0, dilation = 1, groups = 1, [bias, weight, init])\nMCConv(layer, dropout_rate)\n\nCreates a traditional Conv layer with MC dropout functionality.  MC Dropout simply means that dropout is activated in both train and test times \n\nReference - Dropout as a bayesian approximation - https://arxiv.org/abs/1506.02142 \n\nThe traditional conv layer is a field in the struct MCConv, so all the  arguments required for the conv layer can be provided, or the layer can  be provided too. The forward pass is the conv operation of the conv layer followed by dropout applied on the resulting activations. \n\ny = dropout(Conv(x), dropout_rate)\n\nFields\n\nlayer: A traditional conv layer \ndropout_rate::AbstractFloat: Dropout rate \n\nArguments\n\nfilter::NTuple{N,Integer}: Kernel dimensions, eg, (5, 5) \nch::Pair{<:Integer,<:Integer}: Input channels => output channels \ndropout_rate::AbstractFloat: Dropout rate \nσ::F=identity: Activation function, defaults to identity\ninit=glorot_normal: Initialization function, defaults to glorot_normal \n\n\n\n\n\n","category":"function"},{"location":"#DeepUncertainty","page":"Home","title":"DeepUncertainty","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for DeepUncertainty.","category":"page"},{"location":"","page":"Home","title":"Home","text":"DeepUncertainty implements techniques generally used to qualtify uncertainty in neural networks. It implements a variety of methods such Monte Carlo Dropout, BatchEnsemble, Bayesian BatchEnsemble, Bayesian Neural Networks. The goal is to have drop-in replacements to convert deterministic layers to Bayesian networks, and also provide examples to help convert custom layers to theor Bayesian counter-parts.","category":"page"},{"location":"","page":"Home","title":"Home","text":"This package is in development as part of the ACED project, funded by ARPA-E DIFFERENTIATE and coordinated by Carnegie Mellon University, in collaboration with Julia Computing, Citrine Informatics, and MIT.","category":"page"}]
}

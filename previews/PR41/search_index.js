var documenterSearchIndex = {"docs":
[{"location":"layers/mclayers/#MC-Layers","page":"MC Dropout","title":"MC Layers","text":"","category":"section"},{"location":"layers/mclayers/","page":"MC Dropout","title":"MC Dropout","text":"Pages=[\"mclayers.md\"]","category":"page"},{"location":"layers/mclayers/","page":"MC Dropout","title":"MC Dropout","text":"The goal of MC Layers is to facilitate converting any layer defined in Flux into its Bayesian counterpart. MC Dropout is a principled tecnique to estimate predictive uncertainty in neural networs. The paper Dropout as a Bayesian Approximation proves that a neural network with dropout applied before every layer is equivalent to a Deep Gaussain Process. We can apply dropout at test time to simulate functions draws in a Gaussian Process for approximate Variational Inference. ","category":"page"},{"location":"layers/mclayers/","page":"MC Dropout","title":"MC Dropout","text":"In simple terms, a network with dropout applied before every layer has a different weight configuration in every forward pass (Note that we also apply dropout at test time, usually dropout is turned off during test time to ensemble the various weight samples implicitly). Each weight configuration can be considered an independent sample from the underlying weight distribution. The mean and variance of these sample predictions are the mean and variance of the predictive distribution. Higher the variance, more the uncertianty. ","category":"page"},{"location":"layers/mclayers/","page":"MC Dropout","title":"MC Dropout","text":"DeepUncertainty.layers.mclayers.MCLayer","category":"page"},{"location":"layers/mclayers/","page":"MC Dropout","title":"MC Dropout","text":"We also implement MC versions of Conv and Dense layers as a drop-in replacement. ","category":"page"},{"location":"layers/mclayers/","page":"MC Dropout","title":"MC Dropout","text":"DeepUncertainty.layers.mclayers.MCDense\nDeepUncertainty.layers.mclayers.MCConv ","category":"page"},{"location":"#DeepUncertainty","page":"Home","title":"DeepUncertainty","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for DeepUncertainty.","category":"page"},{"location":"","page":"Home","title":"Home","text":"DeepUncertainty implements techniques generally used to qualtify uncertainty in neural networks. It implements a variety of methods such Monte Carlo Dropout, BatchEnsemble, Bayesian BatchEnsemble, Bayesian Neural Networks. The goal is to have drop-in replacements for Dense and Conv layers to immediatly convert deterministic networks to Bayesian networks, and also provide examples to help convert custom layers to Bayesian.","category":"page"},{"location":"","page":"Home","title":"Home","text":"This package is in development as part of the ACED project, funded by ARPA-E DIFFERENTIATE and coordinated by Carnegie Mellon University, in collaboration with Julia Computing, Citrine Informatics, and MIT.","category":"page"}]
}

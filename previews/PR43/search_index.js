var documenterSearchIndex = {"docs":
[{"location":"layers/mclayers/#MC-Layers","page":"MC Dropout","title":"MC Layers","text":"","category":"section"},{"location":"layers/mclayers/","page":"MC Dropout","title":"MC Dropout","text":"Pages=[\"mclayers.md\"]","category":"page"},{"location":"layers/mclayers/","page":"MC Dropout","title":"MC Dropout","text":"The goal of MC Layers is to facilitate converting any layer defined in Flux into its Bayesian counterpart. MC Dropout is a principled tecnique to estimate predictive uncertainty in neural networs. The paper Dropout as a Bayesian Approximation proves that a neural network with dropout applied before every layer is equivalent to a Deep Gaussain Process. We can apply dropout at test time to simulate functions draws in a Gaussian Process for approximate Variational Inference. ","category":"page"},{"location":"layers/mclayers/","page":"MC Dropout","title":"MC Dropout","text":"In simple terms, a network with dropout applied before every layer has a different weight configuration in every forward pass (Note that we also apply dropout at test time, usually dropout is turned off during test time to ensemble the various weight samples implicitly). Each weight configuration can be considered an independent sample from the underlying weight distribution. The mean and variance of these sample predictions are the mean and variance of the predictive distribution. Higher the variance, more the uncertianty. ","category":"page"},{"location":"layers/mclayers/","page":"MC Dropout","title":"MC Dropout","text":"DeepUncertainty.MCLayer","category":"page"},{"location":"layers/mclayers/#DeepUncertainty.MCLayer","page":"MC Dropout","title":"DeepUncertainty.MCLayer","text":"MCLayer(layer, dropout)\n\nA generic Monte Carlo dropout layer. Takes in any \"traditional\" flux  layer and a function that implements dropout. Performs the usual layer  forward pass and then passes the acitvations through the given dropout function.  Let x be the input array, the forward pass then becomes – dropout(layer(x))\n\nFields\n\nlayer: Traditional Flux layer \ndropout: A function that implements dropout \n\n\n\n\n\n","category":"type"},{"location":"layers/mclayers/","page":"MC Dropout","title":"MC Dropout","text":"We also implement MC versions of Conv and Dense layers as a drop-in replacement. ","category":"page"},{"location":"layers/mclayers/","page":"MC Dropout","title":"MC Dropout","text":"DeepUncertainty.MCDense\nDeepUncertainty.MCConv ","category":"page"},{"location":"layers/mclayers/#DeepUncertainty.MCDense","page":"MC Dropout","title":"DeepUncertainty.MCDense","text":"MCDense(in::Int, out::Int, dropout_rate, σ=identity; bias=true, init=glorot_uniform)\nMCDense(layer, dropout_rate)\n\nCreates a traditional dense layer with MC dropout functionality.  MC Dropout simply means that dropout is activated in both train and test times  Reference - Dropout as a bayesian approximation - https://arxiv.org/abs/1506.02142 \n\nThe traditional dense layer is a field in the struct MCDense, so all the  arguments required for the dense layer can be provided, or the layer can  be provided too. The forward pass is the affine transformation of the dense layer followed by dropout applied on the resulting activations. \n\ny = dropout(σ.(W * x .+ bias), dropout_rate)\n\nFields\n\nlayer: A traditional dense layer \ndropout: A function that implements dropout  \n\nArguments\n\nin::Integer: Input dimension of features \nout::Integer: Output dimension of features \ndropout_rate::AbstractFloat: Dropout rate \nσ::F=identity: Activation function, defaults to identity\ninit=glorot_normal: Initialization function, defaults to glorot_normal \n\n\n\n\n\n","category":"function"},{"location":"layers/mclayers/#DeepUncertainty.MCConv","page":"MC Dropout","title":"DeepUncertainty.MCConv","text":"MCConv(filter, in => out, σ = identity;\n        stride = 1, pad = 0, dilation = 1, groups = 1, [bias, weight, init])\nMCConv(layer, dropout_rate)\n\nCreates a traditional Conv layer with MC dropout functionality.  MC Dropout simply means that dropout is activated in both train and test times \n\nReference - Dropout as a bayesian approximation - https://arxiv.org/abs/1506.02142 \n\nThe traditional conv layer is a field in the struct MCConv, so all the  arguments required for the conv layer can be provided, or the layer can  be provided too. The forward pass is the conv operation of the conv layer followed by dropout applied on the resulting activations. \n\ny = dropout(Conv(x), dropout_rate)\n\nFields\n\nlayer: A traditional conv layer \ndropout_rate::AbstractFloat: Dropout rate \n\nArguments\n\nfilter::NTuple{N,Integer}: Kernel dimensions, eg, (5, 5) \nch::Pair{<:Integer,<:Integer}: Input channels => output channels \ndropout_rate::AbstractFloat: Dropout rate \nσ::F=identity: Activation function, defaults to identity\ninit=glorot_normal: Initialization function, defaults to glorot_normal \n\n\n\n\n\n","category":"function"},{"location":"layers/batchensemble/#BatchEnsemble-layers","page":"BatchEnsemble","title":"BatchEnsemble layers","text":"","category":"section"},{"location":"layers/batchensemble/","page":"BatchEnsemble","title":"BatchEnsemble","text":"Pages=[\"batchensemble.md\"]","category":"page"},{"location":"layers/batchensemble/","page":"BatchEnsemble","title":"BatchEnsemble","text":"DeepUncertainty.DenseBE","category":"page"},{"location":"layers/batchensemble/#DeepUncertainty.DenseBE","page":"BatchEnsemble","title":"DeepUncertainty.DenseBE","text":"DenseBE(in, out, rank, \n        ensemble_size, \n        σ=identity; \n        bias=true,\n        init=glorot_normal, \n        alpha_init=glorot_normal, \n        gamma_init=glorot_normal)\nDenseBE(layer, alpha, gamma, ensemble_bias, ensemble_act, rank)\n\nCreates a dense BatchEnsemble layer. Batch ensemble is a memory efficient alternative  for deep ensembles. In deep ensembles, if the ensemble size is N, N different models  are trained, making the time and memory complexity O(N * complexity of one network).  BatchEnsemble generates weight matrices for each member in the ensemble using a  couple of rank 1 vectors R (alpha), S (gamma), RS' and multiplying the result with  weight matrix W element wise. We also call R and S as fast weights. \n\nReference - https://arxiv.org/abs/2002.06715 \n\nDuring both training and testing, we repeat the samples along the batch dimension  N times, where N is the ensemble_size. For example, if each mini batch has 10 samples  and our ensemble size is 4, then the actual input to the layer has 40 samples.  The output of the layer has 40 samples as well, and each 10 samples can be considered  as the output of an esnemble member. \n\nFields\n\nlayer: The dense layer which transforms the pertubed input to output \nalpha: The first Fast weight of size (indim, ensemblesize)\ngamma: The second Fast weight of size (outdim, ensemblesize)\nensemble_bias: Bias added to the ensemble output, separate from dense layer bias \nensemble_act: The activation function to be applied on ensemble output \nrank: Rank of the fast weights (rank > 1 doesn't work on GPU for now)\n\nArguments\n\nin::Integer: Input dimension of features \nout::Integer: Output dimension of features \nrank::Integer: Rank of the fast weights \nensemble_size::Integer: Number of models in the ensemble \nσ::F=identity: Activation of the dense layer, defaults to identity\ninit=glorot_normal: Initialization function, defaults to glorot_normal \nalpha_init=glorot_normal: Initialization function for the alpha fast weight,                       defaults to glorot_normal \ngamma_init=glorot_normal: Initialization function for the gamma fast weight,                        defaults to glorot_normal \nbias::Bool=true: Toggle the usage of bias in the dense layer \nensemble_bias::Bool=true: Toggle the usage of ensemble bias \nensemble_act::F=identity: Activation function for enseble outputs \n\n\n\n\n\n","category":"type"},{"location":"layers/batchensemble/","page":"BatchEnsemble","title":"BatchEnsemble","text":"DeepUncertainty.ConvBE","category":"page"},{"location":"layers/batchensemble/#DeepUncertainty.ConvBE","page":"BatchEnsemble","title":"DeepUncertainty.ConvBE","text":"ConvBE(filter, in => out, rank, \n        ensemble_size, σ = identity;\n        stride = 1, pad = 0, dilation = 1, \n        groups = 1, [bias, weight, init])\nConvBE(layer, alpha, gamma, ensemble_bias, ensemble_act, rank)\n\nCreates a conv BatchEnsemble layer. Batch ensemble is a memory efficient alternative  for deep ensembles. In deep ensembles, if the ensemble size is N, N different models  are trained, making the time and memory complexity O(N * complexity of one network).  BatchEnsemble generates weight matrices for each member in the ensemble using a  couple of rank 1 vectors R (alpha), S (gamma), RS' and multiplying the result with  weight matrix W element wise. We also call R and S as fast weights. \n\nReference - https://arxiv.org/abs/2002.06715 \n\nDuring both training and testing, we repeat the samples along the batch dimension  N times, where N is the ensemble_size. For example, if each mini batch has 10 samples  and our ensemble size is 4, then the actual input to the layer has 40 samples.  The output of the layer has 40 samples as well, and each 10 samples can be considered  as the output of an esnemble member. \n\nFields\n\nlayer: The dense layer which transforms the pertubed input to output \nalpha: The first Fast weight of size (indim, ensemblesize)\ngamma: The second Fast weight of size (outdim, ensemblesize)\nensemble_bias: Bias added to the ensemble output, separate from dense layer bias \nensemble_act: The activation function to be applied on ensemble output \nrank: Rank of the fast weights (rank > 1 doesn't work on GPU for now)\n\nArguments\n\nfilter::NTuple{N,Integer}: Kernel dimensions, eg, (5, 5) \nch::Pair{<:Integer,<:Integer}: Input channels => output channels \nrank::Integer: Rank of the fast weights \nensemble_size::Integer: Number of models in the ensemble \nσ::F=identity: Activation of the dense layer, defaults to identity\ninit=glorot_normal: Initialization function, defaults to glorot_normal \nalpha_init=glorot_normal: Initialization function for the alpha fast weight,                           defaults to glorot_normal \ngamma_init=glorot_normal: Initialization function for the gamma fast weight,                            defaults to glorot_normal \nbias::Bool=true: Toggle the usage of bias in the dense layer \nensemble_bias::Bool=true: Toggle the usage of ensemble bias \nensemble_act::F=identity: Activation function for enseble outputs \n\n\n\n\n\n","category":"type"},{"location":"layers/bayesianbe/#Baysian-BatchEnsemble-layers","page":"Bayesian BatchEnsemble","title":"Baysian BatchEnsemble layers","text":"","category":"section"},{"location":"layers/bayesianbe/","page":"Bayesian BatchEnsemble","title":"Bayesian BatchEnsemble","text":"Pages=[\"bayesianbe.md\"]","category":"page"},{"location":"layers/bayesianbe/","page":"Bayesian BatchEnsemble","title":"Bayesian BatchEnsemble","text":"DeepUncertainty.VariationalDenseBE","category":"page"},{"location":"layers/bayesianbe/#DeepUncertainty.VariationalDenseBE","page":"Bayesian BatchEnsemble","title":"DeepUncertainty.VariationalDenseBE","text":"VariationalDenseBE(in, out, rank, \n                    ensemble_size, \n                    σ=identity; \n                    bias=true,\n                    init=glorot_normal, \n                    alpha_init=glorot_normal, \n                    gamma_init=glorot_normal)\nVariationalDenseBE(layer, alpha_sampler, gamma_sampler,\n                    ensemble_bias, ensemble_act, rank)\n\nCreates a bayesian dense BatchEnsemble layer.  Batch ensemble is a memory efficient alternative for deep ensembles. In deep ensembles,  if the ensemble size is N, N different models are trained, making the time and memory  complexity O(N * complexity of one network).  BatchEnsemble generates weight matrices for each member in the ensemble using a  couple of rank 1 vectors R (alpha), S (gamma), RS' and multiplying the result with  weight matrix W element wise. We also call R and S as fast weights. In the bayesian  version of batch ensemble, instead of having point estimates of the fast weights, we  sample them form a trainable parameterized distribution. \n\nReference - https://arxiv.org/abs/2005.07186\n\nDuring both training and testing, we repeat the samples along the batch dimension  N times, where N is the ensemble_size. For example, if each mini batch has 10 samples  and our ensemble size is 4, then the actual input to the layer has 40 samples.  The output of the layer has 40 samples as well, and each 10 samples can be considered  as the output of an esnemble member. \n\nFields\n\nlayer: The dense layer which transforms the pertubed input to output \nalpha_sampler: Sampler for the first Fast weight of size (indim, ensemblesize)\ngamma_sampler: Sampler for the second Fast weight of size (outdim, ensemblesize)\nensemble_bias: Bias added to the ensemble output, separate from dense layer bias \nensemble_act: The activation function to be applied on ensemble output \nrank: Rank of the fast weights (rank > 1 doesn't work on GPU for now)\n\nArguments\n\nin::Integer: Input dimension of features \nout::Integer: Output dimension of features \nrank::Integer: Rank of the fast weights \nensemble_size::Integer: Number of models in the ensemble \nσ::F=identity: Activation of the dense layer, defaults to identity\ninit=glorot_normal: Initialization function, defaults to glorot_normal \nbias::Bool=true: Toggle the usage of bias in the dense layer \nensemble_bias::Bool=true: Toggle the usage of ensemble bias \nensemble_act::F=identity: Activation function for enseble outputs \nalpha_init=TrainableMvNormal: Initialization function for the alpha fast weight,                       defaults to TrainableMvNormal \ngamma_init=TrainableMvNormal: Initialization function for the gamma fast weight,                        defaults to TrainableMvNormal \n\n\n\n\n\n","category":"type"},{"location":"layers/bayesianbe/","page":"Bayesian BatchEnsemble","title":"Bayesian BatchEnsemble","text":"DeepUncertainty.VariationalConvBE","category":"page"},{"location":"layers/bayesianbe/#DeepUncertainty.VariationalConvBE","page":"Bayesian BatchEnsemble","title":"DeepUncertainty.VariationalConvBE","text":"VariationalConvBE(filter, in => out, rank, \n                ensemble_size, σ = identity;\n                stride = 1, pad = 0, dilation = 1, \n                groups = 1, [bias, weight, init])\nVariationalConvBE(layer, alpha_sampler, gamma_sampler,\n                ensemble_bias, ensemble_act, rank)\n\nCreates a bayesian conv BatchEnsemble layer.  Batch ensemble is a memory efficient alternative for deep ensembles. In deep ensembles,  if the ensemble size is N, N different models are trained, making the time and memory  complexity O(N * complexity of one network).  BatchEnsemble generates weight matrices for each member in the ensemble using a  couple of rank 1 vectors R (alpha), S (gamma), RS' and multiplying the result with  weight matrix W element wise. We also call R and S as fast weights. In the bayesian  version of batch ensemble, instead of having point estimates of the fast weights, we  sample them form a trainable parameterized distribution. \n\nReference - https://arxiv.org/abs/2005.07186\n\nDuring both training and testing, we repeat the samples along the batch dimension  N times, where N is the ensemble_size. For example, if each mini batch has 10 samples  and our ensemble size is 4, then the actual input to the layer has 40 samples.  The output of the layer has 40 samples as well, and each 10 samples can be considered  as the output of an esnemble member. \n\nFields\n\nlayer: The conv layer which transforms the pertubed input to output \nalpha_sampler: Sampler for the first Fast weight of size (indim, ensemblesize)\ngamma_sampler: Sampler for the second Fast weight of size (outdim, ensemblesize)\nensemble_bias: Bias added to the ensemble output, separate from conv layer bias \nensemble_act: The activation function to be applied on ensemble output \nrank: Rank of the fast weights (rank > 1 doesn't work on GPU for now)\n\nArguments\n\nfilter::NTuple{N,Integer}: Kernel dimensions, eg, (5, 5) \nch::Pair{<:Integer,<:Integer}: Input channels => output channels \nrank::Integer: Rank of the fast weights \nensemble_size::Integer: Number of models in the ensemble \nσ::F=identity: Activation of the dense layer, defaults to identity\ninit=glorot_normal: Initialization function, defaults to glorot_normal \nbias::Bool=true: Toggle the usage of bias in the dense layer \nensemble_bias::Bool=true: Toggle the usage of ensemble bias \nensemble_act::F=identity: Activation function for enseble outputs \nalpha_init=TrainableMvNormal: Initialization function for the alpha fast weight,                       defaults to TrainableMvNormal \ngamma_init=TrainableMvNormal: Initialization function for the gamma fast weight,                        defaults to TrainableMvNormal \n\n\n\n\n\n","category":"type"},{"location":"layers/variational/#Variational-Inference-layers","page":"Variational Inference","title":"Variational Inference layers","text":"","category":"section"},{"location":"layers/variational/","page":"Variational Inference","title":"Variational Inference","text":"Pages=[\"variational.md\"]","category":"page"},{"location":"layers/variational/","page":"Variational Inference","title":"Variational Inference","text":"DeepUncertainty.VariationalDense","category":"page"},{"location":"layers/variational/#DeepUncertainty.VariationalDense","page":"Variational Inference","title":"DeepUncertainty.VariationalDense","text":"VariationalDense(in, out, σ=identity;\n                weight_init=TrainableDistribution, \n                bias_init=TrainableDistribution, \n                bias=true)\nVariationalDense(weight_sampler, bias_sampler, act)\n\nCreates a variational dense layer. Computes variational bayesian  approximation to the distribution over the parameters of the dense layer.  The stochasticity is during the forward pass, instead of using point  estimates for weights and biases, we sample from the distribution over  weights and biases. Gradients of the distribution's learnable parameters  are trained using the reparameterization trick.\n\nReference - https://arxiv.org/abs/1505.05424  We use DistributionsAD - https://github.com/TuringLang/DistributionsAD.jl  to help us with backprop. \n\nFields\n\nweight_sampler: A trainable distribution from which weights are sampled                    in every forward pass \nbias_sampler: A trainable distribution from which biases are sampled in                    every forward pass \nact: Activation function, applies to logits after layer transformation \n\nArguents\n\nin::Integer: Input dimension size \nout::Integer: Output dimension size \nσ: Acivation function, defaults to identity \ninit: Distribution parameters Initialization, defaults to glorot_normal\nweight_dist: Weight distribution, defaults to a trainable multivariate normal\nbias_dist: Bias distribution, defaults to trainable multivariate normal\n\n\n\n\n\n","category":"type"},{"location":"layers/variational/","page":"Variational Inference","title":"Variational Inference","text":"DeepUncertainty.VariationalConv","category":"page"},{"location":"layers/variational/#DeepUncertainty.VariationalConv","page":"Variational Inference","title":"DeepUncertainty.VariationalConv","text":"VariationalConv(filter, in => out, σ = identity;\n                stride = 1, pad = 0, dilation = 1, \n                groups = 1, \n                weight_dist = TrainableMvNormal, \n                bias_dist = TrainableMvNormal,\n                [bias, weight, init])\nVariationalConvBE(σ, weight_sampler, bias_sampler,\n                stride, pad, dilation, groups)\n\nCreates a variational conv layer. Computes variational bayesian  approximation to the distribution over the parameters of the conv layer.  The stochasticity is during the forward pass, instead of using point  estimates for weights and biases, we sample from the distribution over  weights and biases. Gradients of the distribution's learnable parameters  are trained using the reparameterization trick.\n\nReference - https://arxiv.org/abs/1505.05424  We use DistributionsAD - https://github.com/TuringLang/DistributionsAD.jl  to help us with backprop. \n\nFields\n\nσ: Activation function, applies to logits after layer transformation \nweight_sampler: A trainable distribution from which weights are sampled                    in every forward pass \nbias_sampler: A trainable distribution from which biases are sampled in                    every forward pass \nstride: Convolution stride \npad\ndilation\ngroups\n\nArguments\n\nfilter::NTuple{N,Integer}: Kernel dimensions, eg, (5, 5) \nch::Pair{<:Integer,<:Integer}: Input channels => output channels \nσ::F=identity: Activation of the dense layer, defaults to identity\nweight_dist=TrainableMvNormal: Initialization function for weights.  \nbias_dist=TrainableMvNormal: Initialization function for biases. \n\n\n\n\n\n","category":"type"},{"location":"initializers/#Bayesian-Layers","page":"Trainable Distributions","title":"Bayesian Layers","text":"","category":"section"},{"location":"initializers/","page":"Trainable Distributions","title":"Trainable Distributions","text":"Pages=[\"initializers.md\"]","category":"page"},{"location":"initializers/","page":"Trainable Distributions","title":"Trainable Distributions","text":"We follow the paper \"Weight Uncertainty in Neural Networks\" to implement Bayesian Layers. The paper proposes replacing point estimates of weights in the weight matrix with trainable distributions. So instead of optimizing for weights directly, we optimize the parameters of the distributions from which we sample weights at every forward pass. The predictive uncertainty is given by approximating the integral over the weight distribution using Monte Carlo sampling. ","category":"page"},{"location":"initializers/","page":"Trainable Distributions","title":"Trainable Distributions","text":"The first component of creating Bayesian layers is trainable distributions, we use DistributionsAD to backprop through distributions using Zygote, the flux autodiff framework.  A trainable distribution should be a subtype of the type AbstractTrainableDist","category":"page"},{"location":"initializers/","page":"Trainable Distributions","title":"Trainable Distributions","text":"DeepUncertainty.TrainableMvNormal","category":"page"},{"location":"initializers/#DeepUncertainty.TrainableMvNormal","page":"Trainable Distributions","title":"DeepUncertainty.TrainableMvNormal","text":"TrainableMvNormal(shape;\n                init=glorot_normal, \n                device=cpu) <: AbstractTrainableDist\nTrainableMvNormal(mean, stddev, sample, shape)\n\nA Multivariate Normal distribution with trainable mean and stddev.\n\nFields\n\nmean: Trainable mean vector of the distribution \nstddev: Trainable standard deviation vector of the distibution \nsample: The latest sample from the distribution, used in calculating loglikelhood loss \nshape::Tuple: The shape of the sample to be returned\n\nArguments\n\nshape::Tuple: The shape of the sample to returned from the distribution \ninit: glorot_normal; to initialize the mean and stddev trainable params \ndevice: cpu; the device to move the sample to, used for convinience while using both GPU and CPU\n\n\n\n\n\n","category":"type"},{"location":"#DeepUncertainty","page":"Home","title":"DeepUncertainty","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for DeepUncertainty.","category":"page"},{"location":"","page":"Home","title":"Home","text":"DeepUncertainty implements techniques generally used to qualtify uncertainty in neural networks. It implements a variety of methods such Monte Carlo Dropout, BatchEnsemble, Bayesian BatchEnsemble, Bayesian Neural Networks. The goal is to have drop-in replacements to convert deterministic layers to Bayesian networks, and also provide examples to help convert custom layers to theor Bayesian counter-parts.","category":"page"},{"location":"","page":"Home","title":"Home","text":"This package is in development as part of the ACED project, funded by ARPA-E DIFFERENTIATE and coordinated by Carnegie Mellon University, in collaboration with Julia Computing, Citrine Informatics, and MIT.","category":"page"}]
}
